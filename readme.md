ğŸš€ G2 Marketing Automation Scraper

Playwright-Based JavaScript-Aware Web Scraper (Python)

ğŸ“Œ Overview

This project is a JavaScript-rendered web scraper built using Playwright (Python) to extract product data from the G2 Marketing Automation category.

Since G2 loads content dynamically, traditional scraping tools like requests + BeautifulSoup fail.
This scraper connects to a real Chrome browser session using CDP (Chrome DevTools Protocol) to extract structured product data reliably. It helps us to protect from captcha also. 

if CAPTCHA appears:

The browser will show the CAPTCHA page.

The script will pause while waiting.

You manually solve the CAPTCHA in the browser window.

After solving it, the page loads normally.

The script continues extracting data.

ğŸ§  Why This Works

Since you're connected to an already running Chrome session:

You can interact with it manually.

You can solve CAPTCHA like a human. And then press enter in terminal.

Once solved, the DOM updates.

Your locators start working again.

Thatâ€™s actually one advantage of using CDP connection instead of headless automation.

ğŸ¯ What This Scraper Extracts

For each product:

âœ… Product Name

âœ… Review Count

âœ… Rating

The data is saved into:

g2_products_full_data.csv

ğŸ§  Why Playwright?

G2 is a JavaScript-heavy platform:

Products are dynamically rendered

Content loads after page initialization

DOM updates asynchronously

ğŸ‘‰ Playwright controls a real Chromium browser, ensuring:

Full JS rendering

Accurate element selection

Better reliability

âš™ï¸ Installation

1ï¸âƒ£ Clone the Repository

git clone https://github.com/Parv-manglani/G2-Products-Scraping-Project.git

cd g2-marketing-automation-scraper

2ï¸âƒ£ Create Virtual Environment

python -m venv venv

Activate it:

Windows

venv\Scripts\activate

Mac/Linux

source venv/bin/activate

3ï¸âƒ£ Install Dependencies

pip install playwright

playwright install

ğŸš€ Running the Scraper

Step 1: Start Chrome in Debug Mode

ğŸªŸ Windows

"C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir="C:\chrome-debug"

If Chrome is in PATH:

chrome --remote-debugging-port=9222 --user-data-dir="C:\chrome-debug"
ğŸ Mac

open -a "Google Chrome" --args --remote-debugging-port=9222 --user-data-dir="/tmp/chrome-debug"

Step 2: Run the Script

python main.py

python whole_data.py

ğŸ” How It Works

1ï¸âƒ£ Connects to Existing Chrome Session

browser = p.chromium.connect_over_cdp("http://localhost:9222")

2ï¸âƒ£ Pagination System

Automatically loops through pages:

while True:
    url = BASE_URL.format(page_number)

Stops when:

No products found

Page number exceeds 30 (safety break)

3ï¸âƒ£ Smart Element Handling

Before extracting data:

if name_locator.count() == 0:
    continue

Prevents crashes due to missing elements.

4ï¸âƒ£ Data Cleaning (Regex)

Removes commas and parentheses from review count:

re.sub(r"[(),]", "", review_text)

5ï¸âƒ£ Deduplication

unique_products = {p["name"]: p for p in products}.values()

Ensures unique product entries.

Then run python whole_data.py

Then see the g2_products_complete_data.csv


ğŸ“Š Output Format

Name	Review Count	Rating     Product Description       Review Summary

Example:

HubSpot Marketing Hub, 10543, 4.4

Marketo Engage, 2321, 4.2

ğŸ”¥ Key Features

âœ”ï¸ JavaScript-aware scraping

âœ”ï¸ Chrome DevTools Protocol connection

âœ”ï¸ Automatic pagination

âœ”ï¸ Timeout handling

âœ”ï¸ Duplicate removal

âœ”ï¸ Clean CSV export

âœ”ï¸ Safety stop to prevent infinite loops
